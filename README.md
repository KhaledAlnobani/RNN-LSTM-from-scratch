#  RNN and LSTM From Scratch (NumPy)

This repository contains a full implementation of **Vanilla Recurrent Neural Network (RNN)** and **Long Short-Term Memory (LSTM)** models built completely from scratch using **NumPy**, including both **forward propagation** and **backpropagation through time (BPTT)**.

The goal of this project is to understand the internal mechanics of sequence models without relying on high-level deep learning frameworks such as TensorFlow or PyTorch.

---

##  Implemented Components

### 1. Vanilla RNN
- Forward propagation through time
- Backward propagation through time (BPTT)
- Hidden state updates
- Gradient computation and parameter updates

### 2. LSTM
- Forget gate, input gate, output gate
- Cell state and hidden state updates
- Forward pass for each time step
- Backward pass with full gradient flow through gates

---
## Acknowledgment

This project is based on the Recurrent Neural Networks and LSTM assignments
from the **Deep Learning Specialization (Sequence Models)** on Coursera by **Andrew Ng**.
The implementation was written from scratch in NumPy for educational and learning purposes.

